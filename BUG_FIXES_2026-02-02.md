# Bug Fixes Applied - Code Review Context Retrieval

## Date: 2026-02-02

## Issues Fixed

### 1. ✅ **Critical: Tree-sitter Parser API Incompatibility**

**Error:**
```
Parser error for python: tree_sitter.Query() takes no arguments
```

**Root Cause:**
The tree-sitter library API changed - `tree_sitter.Query()` constructor no longer accepts arguments in newer versions. The correct API is `language.query(query_scm)`.

**Files Fixed:**
- `src/parser.py` (both `parse_file` and `parse_code` methods)

**Solution:**
Updated parser to:
1. First try `language.query(query_scm)` (correct API)
2. Fall back to `tree_sitter.Query(language, query_scm)` for older versions
3. Catch `TypeError` if Query() doesn't accept arguments
4. Log error and return None gracefully

**Code:**
```python
try:
    query = language.query(query_scm)
    captures = query.captures(tree.root_node)
except (AttributeError, TypeError):
    # Fallback for older tree-sitter versions
    import tree_sitter
    try:
        query = tree_sitter.Query(language, query_scm)
        captures = query.captures(tree.root_node)
    except TypeError:
        logger.error(f"Tree-sitter API incompatibility for {lang_name}...")
        return None, None
```

---

### 2. ✅ **Critical: Variable Used Before Assignment in Analyzer**

**Error:**
```
Error finding related code: cannot access local variable 'file_entities' 
where it is not associated with a value
```

**Root Cause:**
In `analyzer.py` line 1006, `file_entities` was referenced in a log statement before being defined on line 1009.

**File Fixed:**
- `src/analyzer.py` - `_find_related_code()` method

**Solution:**
Moved the log statement to after the variable assignment:

```python
# BEFORE (WRONG):
logger.info(f"[RELATED_CODE] Found {len(file_entities)} entities in {filename}")
file_entities = self.graph_db.find_related_by_file(repo_id, filename, limit=20)

# AFTER (FIXED):
file_entities = self.graph_db.find_related_by_file(repo_id, filename, limit=20)
logger.info(f"[RELATED_CODE] Found {len(file_entities)} entities in {filename}")
```

---

## Remaining Issues (Not Code Bugs)

### 3. ⚠️ **Repository Not Ingested**

**Warning:**
```
No GraphRAG data found for mohd-abex/ai-assesser-abex. 
Repository may not have been ingested yet.
```

**This is NOT a bug** - it's expected behavior when a repository hasn't been ingested yet.

**Solution:**
Run ingestion for your repository:

```bash
curl -X POST http://localhost:8000/ingest \
  -H 'Content-Type: application/json' \
  -d '{
    "repo_url": "https://github.com/mohd-abex/ai-assesser-abex.git",
    "repo_id": "mohd-abex/ai-assesser-abex",
    "installation_id": "107295521"
  }'
```

**Verify ingestion:**
```bash
cd ai-service
python quick_check_ingestion.py mohd-abex/ai-assesser-abex
```

---

### 4. ⚠️ **Gemini API Rate Limit / Quota Issue**

**Error:**
```
429 RESOURCE_EXHAUSTED. You exceeded your current quota, please check 
your plan and billing details.
```

**Analysis:**
Despite you saying the key is not exhausted, the API is returning 429 errors. This could be:

1. **Rate Limit (RPM)** - Not quota, but requests per minute limit hit
2. **Model-specific limits** - `gemini-2.5-pro` has stricter limits than Flash
3. **Billing issues** - API key might need payment method verified
4. **Free tier exhausted** - Free tier has daily/monthly caps

**Solutions:**

#### Option A: Use a Different Model (Immediate Fix)
The system selected `gemini-2.5-pro` for deep review. Flash models have higher limits:

**Temporary workaround** - Force flash-lite model:
```python
# In workflow.py _route_node() method
# Change model selection logic to prefer flash-lite
state["selected_model"] = self.gemini_client.flash_lite_model
```

#### Option B: Verify API Key
1. Go to https://ai.dev/rate-limit to check your quota
2. Go to https://ai.google.dev/gemini-api/docs/rate-limits for limits
3. Check if payment method is added (even for free tier verification)

#### Option C: Add Better Rate Limit Handling
The current retry logic (3 attempts with exponential backoff) isn't helping because it's a quota issue, not a transient error.

**Better approach:**
1. Detect 429 errors specifically
2. Fall back to a lighter model (flash-lite instead of pro)
3. Reduce context size to fit within limits
4. Queue requests if hitting RPM limits

---

## Impact of Fixes

### Before Fixes:
- ❌ Parser completely broken (0 captures for all files)
- ❌ Analyzer crashes with variable error
- ❌ Temporary GraphRAG fails (0 nodes, 0 vectors)
- ❌ No context retrieved from permanent databases
- ❌ Review generation hits quota limit

### After Fixes:
- ✅ Parser should work correctly
- ✅ Analyzer won't crash
- ✅ Temporary GraphRAG should parse files
- ⚠️ Still need to run ingestion for permanent data
- ⚠️ Need to address Gemini quota/rate limit

---

## Testing the Fixes

### 1. Test Parser Fix
```bash
cd ai-service
source .venv/bin/activate

python3 -c "
from src.parser import UniversalParser
parser = UniversalParser()

# Test parsing a Python file
captures, code = parser.parse_file('src/api.py')
print(f'Captures: {len(captures) if captures else 0}')
print(f'Parser working: {captures is not None}')
"
```

**Expected output:**
```
Captures: 50+ (actual number will vary)
Parser working: True
```

### 2. Test Full PR Review (with ingestion)
```bash
# First, run ingestion
curl -X POST http://localhost:8000/ingest \
  -H 'Content-Type: application/json' \
  -d '{
    "repo_url": "https://github.com/mohd-abex/ai-assesser-abex.git",
    "repo_id": "mohd-abex/ai-assesser-abex",
    "installation_id": "107295521"
  }'

# Wait for ingestion to complete (check logs)
# Then test review again
```

---

## Next Steps

1. **Restart your AI service** to load the fixed code:
   ```bash
   # Stop current service (Ctrl+C)
   # Restart
   cd ai-service
   source .venv/bin/activate
   python -m uvicorn src.api:app --reload --port 8000
   ```

2. **Test the parser** with the command above

3. **Run ingestion** for your repository

4. **Address Gemini quota issue:**
   - Check your API key at https://ai.dev/rate-limit
   - Consider using flash-lite model for testing
   - Add payment method if needed (even for free tier)

5. **Try PR review again** after ingestion completes

---

## Files Changed

- ✅ `ai-service/src/parser.py` - Fixed tree-sitter Query API
- ✅ `ai-service/src/analyzer.py` - Fixed file_entities variable order

---

## Monitoring

After restart, watch for these log patterns:

**Good signs (parser working):**
```
[TempGraph] Processing file: ... (python)
[TempGraph] Found 12 captures for ...
[TempGraphRAG] Built temporary GraphRAG: 6 files, 48 nodes, 48 vectors
```

**Bad signs (still broken):**
```
Parser error for python: tree_sitter.Query() takes no arguments
[TempGraph] No captures found for ...
```

---

## Summary

The **critical code bugs are now fixed**. The parser and analyzer should work correctly. However:

- You still need to run **ingestion** for permanent GraphRAG data
- You need to resolve the **Gemini API quota/rate limit** issue separately

The parser fix will enable temporary GraphRAG to work during reviews, which provides basic context even without ingestion. But for full functionality (dependencies, similar code, impact analysis), run ingestion.
